{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fdff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf6477d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image_Array, random_chance=5):\n",
    "    images=[]\n",
    "    import random\n",
    "    for image in image_Array:\n",
    "        noisy = []\n",
    "        for row in image:\n",
    "            new_row =[]\n",
    "            for pix in row:\n",
    "                if random.choice(range(100))<=random_chance:\n",
    "                    new_val = random.uniform(0,1)\n",
    "                    new_row.append(new_val)\n",
    "                else:\n",
    "                    new_row.append(pix)\n",
    "            noisy.append(new_row)\n",
    "        images.append(np.array(noisy))\n",
    "    return np.array(images)\n",
    "\n",
    "# add_noise = np.vectorizerise(add_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba0a817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(trainx,trainy),(testx,testy)= keras.datasets.mnist.load_data()\n",
    "trainx = trainx/255\n",
    "testx = testx/255\n",
    "noisy_trainx = add_noise(trainx)\n",
    "# plt.imshow(trainx[2],cmap=\"gray\")\n",
    "# plt.imshow(noisy_trainx[0], cmap = gray)\n",
    "x_train_noisy = noisy_trainx\n",
    "x_test_noisy = add_noise(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780de41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c881c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21a16463e50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(noisy_trainx[2], cmap = 'gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(trainx[2],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb26f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_train_noisy = np.clip(trainx, 0., 1.)\n",
    "# x_test_noisy = np.clip(testx, 0., 1.)\n",
    "plt.imshow(x_train_noisy[6],cmap=\"gray\")\n",
    "# plt.imshow(trainx[6],cmap=\"gray\")\n",
    "print(x_train_noisy[6].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29eaa6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_47 (Depthwi (None, 28, 28, 1)         10        \n",
      "_________________________________________________________________\n",
      "encode_1 (Conv2D)            (None, 28, 28, 32)        32        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_48 (Depthwi (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "encode_2 (Conv2D)            (None, 28, 28, 64)        2048      \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_49 (Depthwi (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "encode_3 (Conv2D)            (None, 28, 28, 256)       16384     \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_50 (Depthwi (None, 28, 28, 256)       2560      \n",
      "_________________________________________________________________\n",
      "decode_3 (Conv2D)            (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_51 (Depthwi (None, 28, 28, 256)       2560      \n",
      "_________________________________________________________________\n",
      "decode_2 (Conv2D)            (None, 28, 28, 64)        16384     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_52 (Depthwi (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "decode_1 (Conv2D)            (None, 28, 28, 32)        2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_18 (UpSampling (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_53 (Depthwi (None, 28, 28, 32)        64        \n",
      "_________________________________________________________________\n",
      "output (Conv2D)              (None, 28, 28, 1)         32        \n",
      "=================================================================\n",
      "Total params: 109,258\n",
      "Trainable params: 109,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "def SeparableConv( x , num_filters =3 ,kernel_size=3, strides=1 , alpha=1.0 ,dilation = 1, Layer_name = \"\" ):\n",
    "    x = tf.keras.layers.DepthwiseConv2D( kernel_size=kernel_size , padding='same' ,kernel_initializer = 'he_normal', dilation_rate=dilation)( x )\n",
    "#     x = tf.keras.layers.BatchNormalization(momentum=0.9997)( x )\n",
    "    if Layer_name != \"\":\n",
    "        x = tf.keras.layers.Conv2D(np.floor( num_filters * alpha ) , kernel_size = ( 1 , 1 ) \n",
    "                                   , kernel_initializer = 'he_normal', strides = strides , use_bias = False \n",
    "                                   , padding='same' , dilation_rate = dilation, activation = \"relu\",\n",
    "                                   name = Layer_name\n",
    "                                   , )( x )\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D(np.floor( num_filters * alpha ) , kernel_size = ( 1 , 1 ) \n",
    "                                   , kernel_initializer = 'he_normal', strides = strides , use_bias = False\n",
    "                                   , padding = 'same' , dilation_rate = dilation,  activation = \"relu\" )( x )  \n",
    "    return x\n",
    "\n",
    "def DEP_SEP_AUTO_ENCODER(input_size=(28,28,1),depth=3,dilation = 1):\n",
    "    features = 32\n",
    "    encoder_input = tf.keras.layers.Input(input_size)\n",
    "    encoder_naming_template = \"encode_\"\n",
    "    decoder_naming_template = \"decode_\"\n",
    "    encode = encoder_input\n",
    "    for i in range(1,depth+1):\n",
    "        encode = SeparableConv( encode, num_filters = features  ,kernel_size=3,\n",
    "                               dilation = dilation, Layer_name = encoder_naming_template+str(i) )\n",
    "        features = features*(i*2)\n",
    "#     return tf.keras.Model(encoder_input,encode)\n",
    "    #bottle neck\n",
    "#     block_2 = SeparableConv(encode,num_filters = features, kernel_size = 2, dilation =2)\n",
    "#     block_2 = tf.keras.layers.Dropout(0.5)(block_2)\n",
    "#     block_2 = SeparableConv(block_2,num_filters = features, kernel_size = 2, dilation =2)\n",
    "#     block_2 = tf.keras.layers.Dense()()\n",
    "    decoder_input = encode\n",
    "    decode = decoder_input\n",
    "    for i in reversed(range(1,depth+1)):\n",
    "        features = features/(i*2)\n",
    "        decode = SeparableConv( decode, num_filters = features ,kernel_size=3,\n",
    "                               dilation = dilation, Layer_name = decoder_naming_template+str(i) )\n",
    "        decode = tf.keras.layers.UpSampling2D(size=(1, 1)) (decode)\n",
    "        \n",
    "    decoder_output = SeparableConv( decode, num_filters = 1 ,kernel_size=1,\n",
    "                               dilation = dilation, Layer_name = \"output\" )\n",
    "    model = tf.keras.Model(encoder_input,decoder_output)\n",
    "    return model\n",
    "model = DEP_SEP_AUTO_ENCODER()\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "729a328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 123s 248ms/step - loss: 0.2357 - val_loss: 0.0803\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 113s 240ms/step - loss: 0.0879 - val_loss: 0.0773\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 113s 241ms/step - loss: 0.0771 - val_loss: 0.0740\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 101s 216ms/step - loss: 0.0766 - val_loss: 0.0749\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 87s 186ms/step - loss: 0.0738 - val_loss: 0.0711\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 84s 179ms/step - loss: 0.0716 - val_loss: 0.0702\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 84s 178ms/step - loss: 0.0714 - val_loss: 0.0698\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 87s 185ms/step - loss: 0.0712 - val_loss: 0.0789\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 85s 182ms/step - loss: 0.0777 - val_loss: 0.0719\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 84s 179ms/step - loss: 0.0731 - val_loss: 0.0716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ccc6fba4c0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_noisy, trainx,\n",
    "epochs=10,\n",
    "batch_size=128,\n",
    "shuffle=True,\n",
    "validation_data=(x_test_noisy, testx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "506a9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"autoencoder_10_epochs.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ac80a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 82s 173ms/step - loss: 0.0705 - val_loss: 0.0713\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 81s 173ms/step - loss: 0.0695 - val_loss: 0.0735\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 81s 174ms/step - loss: 0.0687 - val_loss: 0.0689\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 81s 173ms/step - loss: 0.0697 - val_loss: 0.0666\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 81s 173ms/step - loss: 0.0714 - val_loss: 0.0672\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 81s 173ms/step - loss: 0.0723 - val_loss: 0.0666\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 81s 173ms/step - loss: 0.0684 - val_loss: 0.0676\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 82s 174ms/step - loss: 0.0702 - val_loss: 0.0682\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 82s 174ms/step - loss: 0.0712 - val_loss: 0.0703\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 82s 176ms/step - loss: 0.0686 - val_loss: 0.0657\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CD61EB69D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('autoencoder_10_epochs.h5'.encode(\"utf-8\"))\n",
    "model.fit(x_train_noisy, trainx,\n",
    "epochs=10,\n",
    "batch_size=128,\n",
    "shuffle=True,\n",
    "validation_data=(x_test_noisy, testx))\n",
    "temp = model.predict(noisy_trainx[0].reshape(-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2ea00fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(\"autoencoder_20_epochs.h5\")\n",
    "\n",
    "# plt.subplot(1,2,1)\n",
    "# # plt.imshow(temp[0],cmap=\"gray\")\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.imshow(noisy_trainx[0],cmap=\"gray\")\n",
    "\n",
    "# plt.imsave(\"a.png\",noisy_trainx[0],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517c45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de2d2ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally at: http://127.0.0.1:7871/\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Interface loading below...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7871/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1cd44a7aee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>, 'http://127.0.0.1:7871/', None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CD44A6D430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CD44A448B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "#user interface for model results\n",
    "import gradio as gr\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def example(image):\n",
    "    model = load_model('autoencoder_10_epochs.h5'.encode(\"utf-8\"))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image\n",
    "    image = image.reshape((-1, 28, 28, 1)),\n",
    "    prediction = model.predict(image)[0]/255\n",
    "    return np.squeeze(prediction, axis=2)\n",
    "image_in = gr.inputs.Image(shape = (28, 28))\n",
    "image_out = gr.outputs.Image(type = \"numpy\")\n",
    "# face = gr.Interface(fn=start, inputs=\"text\", outputs=\"text\")\n",
    "# face.launch()\n",
    "gr.Interface(fn = example,inputs = image_in,outputs = \"image\",\n",
    "             title=\"Image Denoising\",description= \"upload image and denoise it\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08dc131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally at: http://127.0.0.1:7860/\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Interface loading below...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7860/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2194a72dc40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>, 'http://127.0.0.1:7860/', None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#user interface for model results\n",
    "import gradio as gr\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def example(image):\n",
    "    model = load_model('autoencoder_20_epochs.h5'.encode(\"utf-8\"))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image\n",
    "    image = image.reshape((-1, 28, 28, 1)),\n",
    "    prediction = model.predict(image)[0]/255\n",
    "    return np.squeeze(prediction, axis=2)\n",
    "image_in = gr.inputs.Image(shape = (28, 28))\n",
    "image_out = gr.outputs.Image(type = \"numpy\")\n",
    "# face = gr.Interface(fn=start, inputs=\"text\", outputs=\"text\")\n",
    "# face.launch()\n",
    "gr.Interface(fn = example,inputs = image_in,outputs = \"image\",\n",
    "             title=\"Image Denoising\",description= \"upload image and denoise it\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8628f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define the convolutional model\n",
    "# import keras.layers as layers\n",
    "# # input_img = keras.Input(shape=(28, 28, 1))\n",
    "# # x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "# # x = layers.MaxPooling2D((2, 2), padding='same')(x) \n",
    "# # encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# # # At this point the representation is (7, 7, 32)\n",
    "# # x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "# # x = layers.UpSampling2D((2, 2))(x)\n",
    "# # x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "# # x = layers.UpSampling2D((2, 2))(x)\n",
    "# # decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "# # autoencoder = keras.Model(input_img, decoded)\n",
    "# autoencoderer = DEP_SEP_AUTO_ENCODER()\n",
    "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Code example (model (autoencoder) validation)\n",
    "autoencoder.fit(x_train_noisy, trainx,\n",
    "epochs=100,\n",
    "batch_size=128,\n",
    "shuffle=True,\n",
    "validation_data=(x_test_noisy, testx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
